# Multi-Agent System Design: Testing and Security Agents

**Date:** 2026-02-07
**Status:** Approved
**Scope:** Cross-project Claude Code skills for automated testing and security auditing

## Overview

This document describes a multi-agent system implemented as Claude Code skills that can work across any Python project (extensible to other languages). Two coordinating agents provide automated testing and security analysis:

- **Testing Agent** (`/agent:test`): Framework-agnostic test generation, execution, and reporting
- **Security Agent** (`/agent:security`): Hybrid threat modeling (STRIDE + OWASP) with hardening recommendations

## Goals

1. **Portability**: Skills work on any project, not just PelotonRacer
2. **Automation**: Generate tests and identify threats without manual configuration
3. **Coordination**: Security findings automatically trigger security test generation
4. **Semi-autonomous**: Auto-modify test files and configs, ask before changing production code
5. **Actionable outputs**: Both human-readable (Markdown) and machine-readable (JSON) reports

## Architecture

### High-Level Design

```
User invokes skill
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Testing Agent   â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Security Agent   â”‚
â”‚  /agent:test     â”‚  invokesâ”‚ /agent:security  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“                            â†“
  generates tests              finds threats
       â†“                            â†“
  runs tests                   suggests hardening
       â†“                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output: JSON + Markdown reports             â”‚
â”‚  docs/testing/YYYY-MM-DD-test-report.{json,md}â”‚
â”‚  docs/security/YYYY-MM-DD-threat-model.{json,md}â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Command Interface

**Testing Agent:**
- `/agent:test` - Full test generation and execution
- `/agent:test --quick` - Run only tests for recently changed files
- `/agent:test --full` - Comprehensive analysis with coverage reporting
- `/agent:test --file <path>` - Target specific file or directory

**Security Agent:**
- `/agent:security` - Complete threat model and vulnerability scan
- `/agent:security --quick` - Quick scan (OWASP only, no STRIDE)
- `/agent:security --full` - Deep analysis with dependency audit
- `/agent:security --file <path>` - Audit specific file or directory

## Testing Agent

### Workflow

**1. Discovery Phase**
- Scans project for existing tests and framework indicators
  - `pytest.ini`, `conftest.py`, `setup.cfg` â†’ pytest
  - `unittest` imports in test files â†’ unittest
  - No tests found â†’ suggests pytest (industry standard)
- Analyzes codebase structure to understand what needs testing
- Respects `--file` flag to focus analysis

**2. Gap Analysis**
- Identifies untested code paths using static analysis
- Prioritizes by risk:
  1. API clients and external integrations
  2. Data models and serialization
  3. Business logic and calculations
  4. UI components and user interactions
- For PelotonRacer example:
  - `src/api/peloton_client.py`: Auth flows, API error handling
  - `src/services/data_manager.py`: JSON merge/dedup logic
  - `src/models/models.py`: Serialization edge cases

**3. Test Generation**
- Creates test files following detected framework conventions
- Generates comprehensive test cases:
  - **Happy path**: Normal operation with valid inputs
  - **Edge cases**: Boundary conditions, empty inputs, None values
  - **Error conditions**: Invalid inputs, API failures, network timeouts
  - **Mocks**: External APIs, file I/O, time-dependent functions
- Framework-specific patterns:
  - Pytest: fixtures, parametrize, markers
  - Unittest: setUp/tearDown, assertRaises
  - Streamlit: session state fixtures, component mocking
- Automatically writes to appropriate location (`tests/` or project convention)

**4. Execution & Reporting**
- Runs generated tests + existing tests using detected framework
- Captures metrics:
  - Pass/fail status for each test
  - Code coverage percentage (line and branch)
  - Execution time
  - Failure details (stack traces, assertion messages)
- On failures:
  - Analyzes stack traces to identify root cause
  - Suggests fixes with code examples
  - Asks for approval before modifying production code

**5. Documentation**
- Writes structured JSON report (see Data Formats section)
- Generates human-readable Markdown summary
- Cross-references security tests to threat IDs

### Capabilities

**Framework Support:**
- Primary: pytest (most common, richest ecosystem)
- Secondary: unittest (standard library)
- Detection logic adapts to project conventions

**Test Types:**
- Unit tests (isolated functions/methods)
- Integration tests (component interactions)
- API mocking (requests, external services)
- Streamlit component tests (session state, UI elements)

**Coverage Analysis:**
- Line coverage via pytest-cov or coverage.py
- Identifies uncovered branches
- Recommends priority tests for uncovered critical paths

## Security Agent

### Workflow

**1. Asset Inventory**
- Maps data flows through the system
  - Authentication: how users/tokens are verified
  - API calls: external services, rate limits, error handling
  - User inputs: forms, file uploads, query parameters
  - Stored data: databases, files, caches
- Identifies trust boundaries
  - User â†’ Application
  - Application â†’ External APIs
  - Application â†’ File System
  - Application â†’ Environment Variables
- For PelotonRacer example:
  - `.env` â†’ auth module â†’ API client â†’ session state â†’ UI
  - User selects ride â†’ API fetch â†’ JSON storage â†’ analytics â†’ chart display

**2. STRIDE Threat Modeling** (Architectural Analysis)

Applies Microsoft's STRIDE framework to identify design-level threats:

- **Spoofing Identity**
  - Can someone impersonate a legitimate user?
  - Are session tokens properly validated?
  - Example: Session hijacking via stolen JWT

- **Tampering with Data**
  - Can data be modified maliciously?
  - Are stored files protected from modification?
  - Example: Attacker modifies `follower_workouts.json` to cheat rankings

- **Repudiation**
  - Can users deny actions they performed?
  - Is there adequate audit logging?
  - Example: No logs of who synced data or when

- **Information Disclosure**
  - Can sensitive data leak?
  - Are secrets properly protected?
  - Example: API tokens appearing in error messages or logs

- **Denial of Service**
  - Can the application be overwhelmed or crashed?
  - Are there rate limits and resource controls?
  - Example: Unbounded API requests drain rate limits

- **Elevation of Privilege**
  - Can users access unauthorized functionality/data?
  - Are there proper access controls?
  - Example: Viewing other users' workout data without permission

**3. OWASP Code Analysis** (Implementation-Level Vulnerabilities)

Scans for OWASP Top 10 vulnerabilities:

- **A01: Broken Access Control**
  - Missing authorization checks
  - Insecure direct object references

- **A02: Cryptographic Failures**
  - Hardcoded secrets
  - Weak password hashing
  - Unencrypted sensitive data

- **A03: Injection**
  - SQL injection (if using databases)
  - Command injection in subprocess calls
  - Path traversal in file operations

- **A04: Insecure Design**
  - Missing security patterns
  - Unsafe defaults

- **A05: Security Misconfiguration**
  - `.env` files committed to git
  - Debug mode in production
  - Unnecessary features enabled

- **A06: Vulnerable Components**
  - Outdated dependencies with known CVEs
  - Uses `pip freeze` or `requirements.txt` to check versions

- **A07: Authentication Failures**
  - Weak password requirements
  - Missing MFA
  - Session fixation vulnerabilities

- **A08: Data Integrity Failures**
  - Insecure deserialization (pickle, yaml.load)
  - Missing integrity checks on downloads

- **A09: Logging Failures**
  - Sensitive data in logs
  - Insufficient logging for security events

- **A10: Server-Side Request Forgery**
  - Unvalidated URLs in API calls

**4. Hardening Recommendations**

Categorizes findings by severity:
- **Critical**: Exploitable vulnerabilities, immediate fix required
- **High**: Significant security risk, fix before deployment
- **Medium**: Defense-in-depth improvements
- **Low**: Best practice enhancements

For each finding:
- Explains the vulnerability in context
- Shows insecure code example
- Provides secure alternative
- Links to relevant documentation (OWASP, CWE)

**Auto-fixes** (semi-autonomous):
- Configuration files: `.env.example`, `.gitignore` rules, security headers
- Documentation: security guidelines, credential rotation reminders
- Asks for approval before modifying production code

**5. Security Test Generation**

Invokes testing agent with security test specifications:
- "Verify API tokens don't appear in logs or error messages"
- "Test that invalid tokens are rejected with 401, not 500"
- "Ensure path traversal attempts in file operations are blocked"
- "Validate that session tokens expire after timeout"

Tests are linked to threat IDs for traceability.

**6. Documentation**

Outputs:
- **JSON**: Structured threat data (see Data Formats section)
- **Markdown**: Executive summary, threat model, action items
- **Threat diagram** (optional): ASCII art showing attack surfaces

## Agent Coordination

### Security â†’ Testing Flow

When security agent identifies threats, it automatically generates security-focused tests:

**Example: Information Disclosure Threat**

1. Security agent finds:
   ```
   THREAT-001: Information Disclosure - Credential Leakage
   File: src/api/peloton_client.py:67
   Issue: Bearer tokens logged during debug operations
   ```

2. Creates threat entry in JSON report with status: `open`

3. Invokes testing agent:
   ```
   /agent:test --security-spec THREAT-001
   ```

4. Testing agent generates `tests/security/test_credential_handling.py`:
   ```python
   def test_tokens_not_in_error_messages():
       """Verify API tokens don't leak in exceptions"""
       # Test implementation

   def test_tokens_not_in_logs(caplog):
       """Verify tokens are scrubbed from logs"""
       # Test implementation
   ```

5. Both agents update reports with cross-references:
   - Security report: `"test_coverage": ["tests/security/test_credential_handling.py"]`
   - Testing report: `"security_threat_id": "THREAT-001"`

### Manual Workflow

Recommended usage pattern:

```bash
# Initial security assessment
/agent:security --full

# Review findings, approve hardening changes
# Agent auto-updates configs, asks about code changes

# Generate comprehensive test suite
/agent:test --full

# During development
/agent:test --file src/new_feature.py

# Before committing sensitive changes
/agent:security --file src/auth/

# Pre-deployment check
/agent:security --full
/agent:test --full
```

## Data Formats

### Testing Report JSON

`docs/testing/YYYY-MM-DD-test-report.json`:

```json
{
  "timestamp": "2026-02-07T10:30:00Z",
  "project": "PelotonRacer",
  "framework": "pytest",
  "summary": {
    "total_tests": 45,
    "passed": 42,
    "failed": 3,
    "skipped": 0,
    "coverage_percent": 78.5,
    "duration_seconds": 12.3
  },
  "tests_generated": [
    {
      "file": "tests/test_peloton_client.py",
      "test_count": 8,
      "reason": "API client had no auth error tests",
      "security_threat_id": "THREAT-001"
    }
  ],
  "failures": [
    {
      "test": "test_invalid_token_handling",
      "file": "tests/test_peloton_client.py:45",
      "error": "AssertionError: Expected 401, got 500",
      "suggested_fix": "Add proper exception handling for invalid tokens in peloton_client.py:67"
    }
  ],
  "coverage": {
    "total_lines": 2340,
    "covered_lines": 1837,
    "uncovered_files": [
      {"file": "src/services/race_analyzer.py", "coverage": 65.2}
    ]
  },
  "recommendations": [
    "Add integration tests for data_manager.py merge logic",
    "Mock Peloton API responses to avoid rate limiting in tests",
    "Increase coverage for race_analyzer.py ranking algorithms"
  ]
}
```

### Security Report JSON

`docs/security/YYYY-MM-DD-threat-model.json`:

```json
{
  "timestamp": "2026-02-07T10:30:00Z",
  "project": "PelotonRacer",
  "scope": ["src/", ".env.example", "app.py"],
  "summary": {
    "total_threats": 12,
    "critical": 1,
    "high": 3,
    "medium": 5,
    "low": 3,
    "mitigated": 2,
    "open": 10
  },
  "threats": [
    {
      "id": "THREAT-001",
      "category": "STRIDE-InformationDisclosure",
      "severity": "HIGH",
      "title": "API credentials could leak in logs",
      "description": "Bearer tokens and session IDs stored in .env are logged during debug operations in peloton_client.py",
      "affected_files": [
        "src/api/peloton_client.py:67",
        "app.py:22"
      ],
      "attack_scenario": "Attacker gains access to application logs (cloud logging, local files, error tracking service) and extracts valid Peloton API tokens",
      "impact": "Account takeover, unauthorized access to user workout data",
      "likelihood": "Medium (requires log access)",
      "mitigation": "Implement credential scrubbing in logging, use structured logging with automatic redaction of 'token', 'password', 'secret' fields",
      "code_example": "# Before:\nlogger.debug(f'Auth header: {headers}')\n# After:\nlogger.debug('Auth header: Bearer [REDACTED]')",
      "references": [
        "OWASP: A09 Logging Failures",
        "CWE-532: Information Exposure Through Log Files"
      ],
      "status": "open",
      "test_coverage": ["tests/security/test_credential_handling.py::test_tokens_not_in_logs"]
    },
    {
      "id": "VULN-001",
      "category": "OWASP-A03-Injection",
      "severity": "MEDIUM",
      "title": "Potential path traversal in data_manager.py",
      "description": "File paths constructed from user input without validation",
      "affected_files": ["src/services/data_manager.py:89"],
      "cwe": "CWE-22",
      "attack_scenario": "Attacker manipulates file paths to read/write files outside data/ directory",
      "mitigation": "Use pathlib.Path.resolve() and validate paths stay within data/ directory",
      "status": "mitigated",
      "mitigation_commit": "abc1234",
      "test_coverage": ["tests/security/test_path_traversal.py"]
    }
  ],
  "attack_surface": {
    "entry_points": [
      "Streamlit UI inputs (ride selection, user search)",
      "Peloton API responses (untrusted external data)",
      "Environment variables (.env file)"
    ],
    "trust_boundaries": [
      "User â†’ Streamlit App",
      "Streamlit App â†’ Peloton API",
      "Streamlit App â†’ File System (data/)",
      "Environment â†’ Application (secrets)"
    ],
    "sensitive_data": [
      "Peloton API tokens (Bearer JWT)",
      "Peloton session IDs",
      "User credentials (if username/password auth)",
      "Workout performance data (PII)",
      "Follower lists"
    ]
  },
  "dependencies": {
    "vulnerable": [
      {
        "package": "requests",
        "version": "2.28.0",
        "cve": "CVE-2023-XXXXX",
        "severity": "HIGH",
        "fix": "Update to requests>=2.31.0"
      }
    ],
    "outdated": ["pandas 1.5.0 -> 2.2.0 available"]
  },
  "recommendations": [
    "Implement rate limiting for Peloton API calls to prevent DoS",
    "Add input validation for all user-provided data (ride IDs, usernames)",
    "Rotate Peloton credentials regularly (30-90 days)",
    "Enable HTTPS-only session cookies if deploying publicly",
    "Add Content Security Policy headers for XSS protection"
  ]
}
```

### Markdown Report Structure

Both agents generate companion `.md` files with human-friendly formatting:

**Testing Report Markdown:**
```markdown
# Test Report - PelotonRacer
**Generated:** 2026-02-07 10:30:00
**Framework:** pytest

## Summary
âœ… 42 passed | âŒ 3 failed | â­ï¸ 0 skipped
ğŸ“Š Coverage: 78.5%
â±ï¸ Duration: 12.3s

## Tests Generated
- `tests/test_peloton_client.py`: 8 tests (auth error handling)
  - Linked to security finding: THREAT-001

## Failures
### test_invalid_token_handling
**File:** tests/test_peloton_client.py:45
**Error:** AssertionError: Expected 401, got 500

**Suggested Fix:**
Add proper exception handling for invalid tokens in peloton_client.py:67

## Recommendations
1. Add integration tests for data_manager.py merge logic
2. Mock Peloton API responses to avoid rate limiting
3. Increase coverage for race_analyzer.py (currently 65%)
```

**Security Report Markdown:**
```markdown
# Security Threat Model - PelotonRacer
**Generated:** 2026-02-07 10:30:00

## Executive Summary
ğŸ”´ 1 Critical | ğŸŸ  3 High | ğŸŸ¡ 5 Medium | ğŸŸ¢ 3 Low
Status: 10 Open, 2 Mitigated

## Critical Threats
None

## High Severity Threats
### THREAT-001: API credentials could leak in logs
**Category:** STRIDE - Information Disclosure
**Files:** src/api/peloton_client.py:67, app.py:22

**Attack Scenario:**
Attacker gains access to logs and extracts valid Peloton tokens

**Mitigation:**
Implement credential scrubbing in logging:
```python
# Before
logger.debug(f'Auth header: {headers}')
# After
logger.debug('Auth header: Bearer [REDACTED]')
```

**Test Coverage:** tests/security/test_credential_handling.py

## Attack Surface
**Entry Points:** Streamlit UI, Peloton API, .env file
**Sensitive Data:** API tokens, session IDs, workout PII

## Recommendations
1. Implement API rate limiting
2. Add input validation for user data
3. Rotate credentials every 30-90 days
```

## Skill File Structure

```
~/.claude/skills/agents/
â”œâ”€â”€ agent-test/
â”‚   â”œâ”€â”€ skill.md                    # Main skill definition (invoked by Claude Code)
â”‚   â”œâ”€â”€ README.md                   # Usage documentation
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”œâ”€â”€ pytest_template.py      # Test generation templates
â”‚   â”‚   â”œâ”€â”€ unittest_template.py
â”‚   â”‚   â”œâ”€â”€ streamlit_fixture.py    # Streamlit session state fixtures
â”‚   â”‚   â””â”€â”€ security_test.py.j2     # Security-specific test templates
â”‚   â”œâ”€â”€ analyzers/
â”‚   â”‚   â”œâ”€â”€ coverage_analyzer.py    # Code coverage analysis
â”‚   â”‚   â”œâ”€â”€ framework_detector.py   # Detect pytest vs unittest
â”‚   â”‚   â””â”€â”€ gap_analyzer.py         # Find untested code paths
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ test_runner.py          # Execute tests, capture results
â”‚       â””â”€â”€ mock_generator.py       # Auto-generate mocks for APIs
â”‚
â”œâ”€â”€ agent-security/
â”‚   â”œâ”€â”€ skill.md                    # Main skill definition
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ threat_models/
â”‚   â”‚   â”œâ”€â”€ stride.py               # STRIDE analysis logic
â”‚   â”‚   â””â”€â”€ owasp.py                # OWASP Top 10 checks
â”‚   â”œâ”€â”€ scanners/
â”‚   â”‚   â”œâ”€â”€ code_scanner.py         # Static analysis for vulnerabilities
â”‚   â”‚   â”œâ”€â”€ config_scanner.py       # .env, .gitignore, CORS checks
â”‚   â”‚   â”œâ”€â”€ dependency_scanner.py   # CVE checking via pip-audit
â”‚   â”‚   â””â”€â”€ secrets_detector.py     # Find hardcoded credentials
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”œâ”€â”€ threat_report.md.j2     # Markdown report template
â”‚   â”‚   â””â”€â”€ security_test.py.j2     # Security test generation
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ severity_calculator.py  # CVSS-like severity scoring
â”‚       â””â”€â”€ mitigation_suggester.py # Secure code examples
â”‚
â””â”€â”€ shared/
    â”œâ”€â”€ report_generator.py         # JSON + Markdown output
    â”œâ”€â”€ project_analyzer.py         # Detect project type, structure
    â””â”€â”€ coordination.py             # Agent-to-agent communication
```

## Implementation Plan

### Phase 1: Testing Agent Foundation
**Goal:** Basic test generation and execution

**Tasks:**
1. Framework detection logic
   - Scan for `pytest.ini`, `conftest.py`, `setup.cfg`
   - Parse test files for unittest vs pytest patterns
   - Default to pytest if no tests exist

2. Simple test generation
   - Parse Python files with AST
   - Generate unit tests for pure functions
   - Use templates for common patterns

3. Test execution
   - Run pytest/unittest via subprocess
   - Capture stdout/stderr
   - Parse test results (pass/fail/skip counts)

4. Basic reporting
   - JSON output with summary stats
   - Markdown summary

**Deliverable:** `/agent:test` works on simple Python projects

### Phase 2: Testing Agent Enhancements
**Goal:** Handle complex projects like PelotonRacer

**Tasks:**
1. Streamlit test support
   - Session state fixtures
   - Component mocking
   - Page navigation tests

2. API mocking
   - Auto-generate mocks for requests calls
   - Parametrized tests for different API responses

3. Coverage analysis
   - Integrate pytest-cov
   - Identify uncovered branches
   - Prioritize test generation

4. Failure analysis
   - Parse stack traces
   - Suggest fixes based on error patterns

**Deliverable:** Comprehensive testing for web apps

### Phase 3: Security Agent Foundation
**Goal:** Basic threat identification

**Tasks:**
1. STRIDE threat modeling
   - Map data flows
   - Identify trust boundaries
   - Generate STRIDE threats for common patterns

2. OWASP code scanning
   - Static analysis for injection vulnerabilities
   - Secrets detection (regex patterns for API keys, passwords)
   - Insecure deserialization checks

3. Config scanning
   - Check `.gitignore` for `.env`
   - Validate `.env.example` exists
   - Flag debug mode in production

4. Basic reporting
   - JSON threat model
   - Markdown summary with severity rankings

**Deliverable:** `/agent:security` produces threat models

### Phase 4: Security Agent Enhancements
**Goal:** Actionable security hardening

**Tasks:**
1. Dependency scanning
   - Parse `requirements.txt`
   - Check for known CVEs (pip-audit integration)
   - Suggest updates

2. Auto-hardening
   - Generate secure `.env.example`
   - Update `.gitignore` with security patterns
   - Add security headers to web apps

3. Mitigation suggestions
   - Code examples for secure alternatives
   - Links to OWASP/CWE documentation
   - Severity-based prioritization

**Deliverable:** Security agent can auto-fix config issues

### Phase 5: Agent Coordination
**Goal:** Security findings trigger security tests

**Tasks:**
1. Cross-referencing
   - Link threats to test files
   - Track mitigation status

2. Security test generation
   - Security agent invokes testing agent
   - Pass threat specifications
   - Generate security-focused regression tests

3. Workflow integration
   - `/agent:security --full` triggers `/agent:test` for security tests
   - Status tracking (open â†’ mitigated â†’ tested)

**Deliverable:** Coordinated security + testing workflow

### Phase 6: Multi-Language Support
**Goal:** Extend beyond Python

**Tasks:**
1. JavaScript/TypeScript support
   - Jest, Vitest framework detection
   - ESLint security rules
   - npm audit integration

2. Go support
   - Go test framework
   - gosec security scanner

3. Language-agnostic patterns
   - Generic STRIDE threat modeling
   - Config file security (regardless of language)

**Deliverable:** Skills work on polyglot projects

## Usage Examples

### Initial Project Setup

```bash
# Clone or start new project
cd /path/to/my-project

# Run comprehensive security audit
/agent:security --full
```

**Output:**
```
ğŸ” Analyzing project structure...
âœ… Detected: Python/Streamlit application
ğŸ“Š Found 15 files, 2,340 lines of code

ğŸ›¡ï¸ Running STRIDE threat modeling...
âš ï¸  Identified 12 threats (1 HIGH, 3 MEDIUM, 8 LOW)

ğŸ”’ Running OWASP code analysis...
âš ï¸  Found 3 vulnerabilities (1 HIGH, 2 MEDIUM)

ğŸ“¦ Scanning dependencies...
âš ï¸  Found 1 outdated package with known CVE

ğŸ“ Writing reports...
âœ… docs/security/2026-02-07-threat-model.json
âœ… docs/security/2026-02-07-threat-model.md

Would you like me to auto-fix configuration issues? (y/n)
```

User approves, agent:
- Adds `.env` to `.gitignore`
- Creates `.env.example` with placeholder values
- Updates `requirements.txt` to fix CVE

```bash
# Generate comprehensive test suite
/agent:test --full
```

**Output:**
```
ğŸ” Detecting test framework...
âœ… Found pytest configuration

ğŸ“Š Analyzing codebase...
âœ… 10 files analyzed, 15 functions need tests

âœï¸  Generating tests...
âœ… tests/test_peloton_client.py (8 tests)
âœ… tests/test_data_manager.py (12 tests)
âœ… tests/security/test_credential_handling.py (4 security tests)

ğŸ§ª Running test suite...
âœ… 42 passed, 3 failed, 78.5% coverage

ğŸ“ Writing reports...
âœ… docs/testing/2026-02-07-test-report.json
âœ… docs/testing/2026-02-07-test-report.md

âŒ 3 tests failed. Would you like me to analyze failures? (y/n)
```

### During Development

```bash
# After adding new authentication method
/agent:security --file src/auth/new_oauth.py
```

**Output:**
```
ğŸ” Analyzing src/auth/new_oauth.py...

âš ï¸  THREAT-013: STRIDE - Spoofing Identity
    OAuth state parameter not validated (CSRF risk)
    Severity: HIGH

    Suggested fix:
    ```python
    # Generate cryptographic random state
    import secrets
    state = secrets.token_urlsafe(32)
    session['oauth_state'] = state
    ```

Would you like me to:
1. Apply this fix automatically
2. Generate security tests for OAuth flow
3. Both
```

```bash
# Quick test run during TDD
/agent:test --file src/services/new_feature.py
```

**Output:**
```
ğŸ” Analyzing src/services/new_feature.py...
âœ… Found 3 new functions

âœï¸  Generating tests...
âœ… tests/test_new_feature.py (6 tests created)

ğŸ§ª Running tests...
âœ… 6 passed, 100% coverage for new code

No issues found. Ready to commit!
```

### Pre-Deployment

```bash
# Comprehensive security + testing check
/agent:security --full && /agent:test --full
```

**Output:**
```
ğŸ›¡ï¸ Security Agent: Running full audit...
âœ… 10 open threats from previous scan
âœ… 2 newly mitigated (THREAT-001, VULN-001)
âš ï¸  1 new threat found (dependency update needed)

Overall security posture: GOOD (2 HIGH, 8 MEDIUM remaining)

ğŸ§ª Testing Agent: Running full test suite...
âœ… 58 tests, 56 passed, 2 failed
ğŸ“Š Coverage: 82.3% (+3.8% since last run)

âš ï¸  2 test failures need attention before deployment.
```

## Success Metrics

**Testing Agent:**
- Code coverage increase after first run
- Number of bugs caught by generated tests
- Reduction in manual test writing time

**Security Agent:**
- Number of vulnerabilities identified
- Time to identify security issues (vs manual review)
- Mitigation completion rate

**Combined:**
- Security tests as % of total tests
- Trend of open threats over time
- Developer adoption rate (uses per week)

## Future Enhancements

### Short-term (3-6 months)
- CI/CD integration (GitHub Actions, GitLab CI)
- Custom rule definitions (user-defined security patterns)
- Performance testing (load tests, profiling)
- Mutation testing (test quality validation)

### Medium-term (6-12 months)
- Multi-language support (JavaScript, Go, Rust)
- Integration with vulnerability databases (NVD, Snyk)
- Automated dependency updates with security patches
- Security metrics dashboard (track posture over time)

### Long-term (12+ months)
- Fuzzing integration (property-based testing)
- Penetration testing guidance (attack simulation)
- Compliance checking (GDPR, SOC2, HIPAA)
- Team collaboration features (shared threat models)

## Risks and Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| False positives in security scanning | Developer fatigue, ignored warnings | Severity tuning, allowlist for known-safe patterns |
| Generated tests don't reflect real usage | Low-quality test suite | Manual review checkpoints, user feedback loop |
| Performance impact on large codebases | Slow analysis | Incremental analysis with `--file`, caching results |
| Breaking changes when auto-fixing code | Introduced bugs | Semi-autonomous mode requires approval, comprehensive testing |
| Dependency on external tools (pip-audit) | Installation friction | Graceful degradation, clear setup instructions |

## Conclusion

This agent system provides automated, cross-project testing and security capabilities as reusable Claude Code skills. The coordinated workflow ensures security findings are validated through tests, creating a robust development process.

**Key Benefits:**
- **Portable**: Works on any Python project (extensible to other languages)
- **Automated**: Minimal manual configuration required
- **Actionable**: Produces both human and machine-readable outputs
- **Coordinated**: Security and testing work together
- **Safe**: Semi-autonomous mode keeps developers in control

**Next Steps:**
1. Implement Phase 1 (Testing Agent Foundation)
2. Test on PelotonRacer as reference implementation
3. Iterate based on real-world usage
4. Publish to skills marketplace for community use
